#!/bin/bash
#
# Use User Scripts (available at the plugins tab in unraid) to
# automatically run the script. It is possible to schedule the
# action.
#
# Basic usage:
#   ./rclone.sh
#
# Every five days this script should run at 04:00 AM
# Custom schedule format (standard cron entry): "0 4 1/5 * *"
#
# Script requires valid credentials - set up with `rclone config`.
#
# Based on https://github.com/geerlingguy/my-backup-plan

# Variables.
# Variables for the first bucket (ClipKiller)
rclone_remoteKnox=personal       #Name of the remote created in the rclone config
rclone_s3_bucketKnox=bucket_name #Name of the bucket generated by AWS
# Variables for the second bucket (Knox)
rclone_remoteClipKiller=personal       #Name of the remote created in the rclone config
rclone_s3_bucketClipKiller=bucket_name #Name of the bucket generated by AWS

bandwidth_limit=23M # 192,937984 Mbps

# Verify the location of rclone using 'whereis' in the terminal
RCLONE=/usr/local/bin/rclone

# Rclone password input
export RCLONE_CONFIG_PASS=secret.password

# Check if rclone is installed.
if ! [ -x "$(command -v $RCLONE)" ]; then
  echo 'Error: rclone is not installed.' >&2
  exit 1
fi

# Make sure bucket exists.
# If it does not exist it will create a new one
$RCLONE mkdir $rclone_remote:$rclone_s3_bucket

# List of directories to clone.
# Place here all the directories that you want to be synced to AWS S3
declare -a dirs=(
  "/Volumes/Media/Movies"
)

# Sync each directory.
for i in "${dirs[@]}"; do
  echo "Syncing Directory: $i"
  despaced="${i// /_}"
  $RCLONE sync "$i" $rclone_remote:$rclone_s3_bucket"$despaced" --skip-links --progress --bwlimit $bandwidth_limit
done

# TODO : Add the second 'for' for the second bucket
